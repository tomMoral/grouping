\documentclass{gretsi}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb}
\usepackage[english,francais]{babel}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
%\usepackage{enumitem}


%Gummi|065|=)
\title{Separation de sources automatique}
\author{Thomas Moreau}
\date{}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother
\def\HH{\mathcal H}

\resumefrancais{Cet article introduit différentes stratégies de groupement automatique pour les composantes issues d'une analyse du spectre singulier (SSA). Cette étape, cruciale à la séparation de composantes haut niveau, permet de séparer les différentes effets présents dans le signal. }
\resumeanglais{This paper introduce automatic grouping strategies for Singular Spectrum Analysis (SSA) components. This step is crucial to get meaningful results}


\newcommand{\R}{\mathbb R}
\newcommand{\serie}[1]{(#1_i)_{1 \le i\le T}}
\newcommand{\val}[3]{(#1_1 #3 \dots #3 #1_#2)}
\newcommand{\set}[1]{\left \{ 1, \dots, #1 \right \}}

\begin{document}

\maketitle


\section{Introduction}

L'analyse de la dynamique sous-jacente d'une série temporelle est très intéressante pour l'étude de phénomènes non stationnaires. En finance ou en météorologie, les séries étudiées sont souvent courtes et bruitées et il n'est pas facile de les analyser. L'extraction de phénomènes de saisonnalité et leur étude peut apporter beaucoup d'information notamment sur les mécanisme produisant ces séries et parfois même permettre de formuler un modèle de prédiction.\\

L'analyse du spectre singulier est une technique très utile pour l'analyse de séries temporelles. Elle permet d'obtenir de manière simple une décomposition du signal en composantes de tendance, de saisonnalité et de bruit. Cette décomposition est utilisée pour l'étude de signaux en finance ou en biologie car elle permet de séparer différents mécanismes présents dans la série de départ, facilitant leur étude.\\

Cependant, cette technique est peu utilisé du fait de son manque d'automatisation. En effet, la plupart des utilisations faites se basent sur la sélection manuelle de certains paramètres, notamment le regroupement de certaines composantes entre elles. Ce regroupement est critique car sans lui, un même phénomène peu se retrouver retrouver sur plusieurs des composantes fournies, ce qui complique l'analyse.\\

Dans cet article, nous étudions différentes stratégies de regroupement présentées dans la littérature. Celles-ci sont comparées à nos propres techniques.

\subsection{SSA}

L'analyse du spectre singulier (SSA) est une technique introduite par Vautard et al \cite{vautard_ghil_89_SSA} pour l'analyse de la dynamique de séries temporelles. Cette technique, basée sur le théorème de Karhunen-Loève, permet de décomposer un signal en une somme de composante haut niveau rattachées à sa tendance et à sa saisonnalité.

\paragraph{Matrices de trajectoires}

On considère un signal fini échantillonné $f = \val{f}{T}{,}$ et un entier $K \in \left \{ 1, \dots, T/2 \right \}$. La matrice de K-trajectoire $X^{(K)} \in \R^{K\times L}$ de la série $f$ est défini par:
$$ X^{(K)} = \begin{bmatrix}
	f_1 & f_2 &\dots & f_K\\
	f_2 & f_3 &\dots & f_{K+1}\\
	\dots\\
	f_{L} & f_{L+1} &\dots & f_T
\end{bmatrix}$$ avec $L = T-K$. Cette matrice contient toutes les sous séries de longueur $K$ de $f$.\\

Le signal peut être reconstruit naturellement à partir de cette matrice du fait de sa structure de Hankel, avec toutes les composantes sur les anti-diagonales égales. Si l'on considère une matrice $M$ n'ayant pas cette structure, le signal ayant la matrice de trajectoire la plus proche de $M$ peut être construit en moyennant le long des anti-diagonales. Cette opération permet de définir un mapping entre l'espace des matrices et celui des signaux. On notera par la suite $\HH: \R^{K\times L} \to \R^T$ l'opération d'hankelisation, qui effectue la moyenne le long des anti-diagonales. Cette opération est linéaire.

\paragraph{Extraction de motifs}

Une décomposition en valeur singulière (SVD) de la matrice de K-trajectoire $X^{(K)}$ de $f$ permet d'extraire les motifs de longueur $K$ les plus intéressants au sens de la variance. En effet, la SVD donne: 
$$
X^{(K)} = U \Lambda V^T
$$ avec $U, V$ deux matrices orthogonales et $\Lambda$ une matrice diagonale ayant pour valeurs $\lambda_1\le \dots \lambda_K$. La matrice $U$ peut être vu comme un dictionnaire de motifs de taille $K$ sur lequel est projeté le signal. Ces motifs permettent de capturer au mieux la variance du signal. Les vecteurs singuliers de la SVD maximisent récursivement la covariance de $U_i$ et des sous séries de longueur $K$ de $f$.

\paragraph{Reconstruction de composantes}

L'étape d'extraction de motif permet d'obtenir une décomposition de la matrice de trajectoire sous la forme d'une somme de matrice de rang 1:$$
X^{(K)} = \sum_{i=1}^K \lambda_i U_iV_i^T 
$$où  $U_i$.Par linéarité de l'opération d'hankelisation, on obtient une décomposition de $f = \sum_{i=1}^K f^{(i)}$ avec $f^{(i)} = \HH(\lambda_iU_iV_i^T)$ la série associée à la matrice de rang 1 $\lambda_iU_iV_i^T$.\\

Les oscillations harmoniques ou plus complexes, présentes dans la série initial sont généralement décomposées sur plusieurs de ces composantes. Le bruit est lui aussi projeté sur plusieurs composantes. Une étape d'identification et de regroupement est alors nécessaire pour produire une décomposition intéréssante. Une sélection des composantes peut être fait en se basant sur les valeurs singulières associées qui rendent compte de l'importance de la composante dans la série. Une étape de regroupement est ensuite nécessaire pour obtenir une décomposition plus simple à analyser.\\

Un regroupement consiste à fournir un ensemble $(I_k)_{1 \le k \le d}$ tel que $\forall k, j$, $I_k \cap I_j = \emptyset$ and $\displaystyle \cup_{k=1}^d I_k = I$. Les composantes ainsi créées sont alors $f^{(I_k)} = \sum_{i \in I_k} f^{(i)}$. Le but de cette étape est de définir les sous espaces sur lesquels sont projetés les sous séries de longueur $K$ de $f$. Si ces sous espace sont bien choisis, les composantes oscillantes se retrouvent sur une unique composante $f^{(I_k)}$. Les auteurs de \cite{GNZ_10_SSA} proposent différents indicateurs pour aider au groupement mais ceux-ci requière une sélection manuelle des composantes à grouper.

\subsection{Automatisation du groupement}
\label{sub:}
Les travaux de Alexandrov et Golyandina \cite{alexandrov_05_auto} propose une première approche pour le regroupement automatique, principalement tourner à la récupération d'harmonique pure dans le signal. Dans \cite{alvarez_2013_auto}, Alvarez et al. propose une seconde méthode de regroupement automatique basé sur le clustering des périodigrammes des composantes reconstruites. Cette méthode vise un regroupement plus général des composantes. Dans des travaux récents, Abalov et Gubarev \cite{abalov_14_auto} présentent deux autres stratégies pour obtenir un regroupement automatique.


% subsection  (end)

\section{Regroupement automatique}

\subsection{Formulation générale}
\label{sub:form}

Les stratégies de regroupement  peuvent être décrite en trois phases:
\begin{enumerate}
	\item Sélectionner des composantes intéressante de la SSA
	\item Calculer matrice d'adjacence entre ces composantes
	\item Regrouper les composantes qui sont adjacentes
\end{enumerate}
L'étape 1 est effectuer en filtrant les composantes de la SVD ayant une valeur singulière $\lambda_k$ plus faible que $\tau_l*\lambda_1$. Ceci permet de filtrer les composantes de bruits en considérant qu'elles ont une part négligeable dans la variance de la série. On fixe pour toutes les expériences $\tau_l = 0.01$.\\

La seconde étape est définie par le choix d'une mesure de similarité entre les composantes. La matrice d'adjacence est calculer à partir de condition sur la similarité des composantes. La similarité choisie impact fortement les résultats du regroupement. Une similarité basée sur le périodigramme donnera de meilleur résultat pour le groupement de composantes purement harmonique que pour le groupement de composantes quasi-périodique. Les différents choix de fonction de similarité sont discutés dans la section \ref{sub:sim}.\\

Enfin, le choix de la stratégie de formation des groupes à partir de la matrice d'adjacence permet de considérer les composantes de manière différente au cours de l'étape 3. Il peut parfois être intéressant d'avoir un élément prévalent dans chaque groupe du fait de l'ordre induit par les valeurs singulières associées aux différentes composantes. La section \ref{sub:clust} discute des différents choix possibles.

% subsection form (end)


\subsection{Stratégie de clustering}
\label{sub:clust}

La stratégie de formation des groupes à partir de la matrice d'adjacence la plus simple est de considéré que toutes les composantes ont un interet uniforme. On peut alors considéré que deux composantes sont dans le même groupe dès lors qu'elles sont adjacentes. L'algorithme \ref{alg:unif} décrite la méthode de formation des groupes dans ce cas la.\\

\begin{algorithm}
\caption{Groupement uniforme}\label{alg:unif}
Given G
\begin{algorithmic}[1]
\For{ i = 1 \dots K}
\State group[i] = i
\EndFor
\For{ i = 1 \dots K}
\For{ j = i+1 \dots K}
\If{$G_{i,j} = 1$} group[j] = group[i]
\EndIf
\EndFor
\EndFor
\State \Return group
\end{algorithmic}
\end{algorithm}

Une autre possibilité est de considéré que les composantes ont une importance différente. L'importance de chaque composante peut être mesurée par la part de variance $\frac{\lambda_i}{\sum_{j=1}^n \lambda_j}$ expliqué par cette composante. On peut donc considéré qu'il est préférable de rajouter une composante $j$ dans un groupe $I_k$ que si $G_{i, j} = 1$ pour $i = \arg\max_{l \in I_k} \lambda_l$. Ici, la composante est ajouté dans un groupe uniquement lorsqu'elle est similaire à la composante la plus importante du groupe.

\begin{algorithm}
\caption{Groupement ordonée}\label{alg:ord}
Given G
\begin{algorithmic}[1]
\State GI $= \left \{  \right \}, I = \set{m}$
\For{$i \in I$}
\State remove i from $I$, $I_i = \left \{ i \right \}$
\For{$j \in I$}
\If{$G_{i,j} = 1$} 
\State remove j from $I$, $I_i = I_i \cup \left \{ j \right \}$
\EndIf
\EndFor
\State GI = GI$ \cup I_i$
\EndFor
\State \Return GI
\end{algorithmic}
\end{algorithm}


% subsection clust (end)

\subsection{Mesures de similarité}
\label{sub:sim}

\subsubsection{Corrélation}

\paragraph{Abalov (GG1)}
\label{par:}
La métrique de similarité proposée par les auteurs de \cite{abalov_14_aut} repose sur deux points. Tout d'abord, le ratio des valeurs singulières associées aux signaux considérés doit être proche 1. Ceci implique que l'on ne peut pas regrouper des composantes trop éloignées, pour éviter de grouper des phénomènes présents dans la séries 'ayant pas les même amplitudes. Cet effet est controlé par un seuil $\rho_1$. Ensuite, deux composantes sont regroupées si la corrélation entre elles est plus haute qu'une valeur seuil $\rho_c$:
$$
\texttt{Sim}(i, j) = \begin{cases}
	1 &\text{ if } \displaystyle\frac{\min(\lambda_i, \lambda_j)}{\max(\lambda_i, \lambda_j)} \ge \rho_1 \text{ et } \text{corr}({(i)}, f^{[j)}) \ge \rho_c\\
	0& \text{ sinon}
\end{cases}
$$
% paragraph  (end)

\paragraph{W-Corrélation (GG3)}
\label{par:nn}

Dans \cite{GNZ_10_SSA}, les auteurs introduisent le concept de w-corrélation pour deux séries $x, y$ de taille $N$ et pour une taille de fenêtre $K$:$$
\text{w-corr}(x, y) = \frac{\langle x|y\rangle_w}{\|x\|_w\|y\|_w}
$$avec $\langle x|y\rangle_w = \sum_{i=1}^n w_i x_i y_i$, $w_i = \min(i, N-i, K)$ et $\|x\|_w = \sqrt{\langle x|x\rangle_w}$. Deux séries sont séparables par la SSA si leur w-corrélation est nulle. Il est donc possible d'utiliser la valeur de la w-corrélation comme indicateur de similarité entre 2 composantes. On peut alors prendre une fonction de similarité identitque à celle de Abalov en remplaçant la corrélation par la w-corrélation. Ceci permet de filtrer les effets de bord qui peuvent apparaître dans la corrélation et ainsi d'obtenir un meilleur regroupement.
% paragraph nn (end)


\subsubsection{Periodigramme}

La fréquence des composantes obtenues peut aussi être utilisé comme un indicateur de similarité des séries. Dans \cite{GNZ_10_SSA}, les auteurs recommandent d'observer le périodigrame des composantes pour inférer le regroupement. Le périodigrame d'une série $f$ de longueur $N$ est définie comme:$$
\Pi_f(k) = \frac{1}{Z}\left|\sum_{n=1}^N f_n e^{-2i\pi n \frac{k}{N}}\right|^2
$$ où $Z$ est une constante de normalisation telle que le périodigramme est une norme unitaire.\\

\paragraph{Regroupement harmonique (HG)}\label{par:alex} Alexandrov \& al. proposent dans \cite{alexandrov_05_auto} une stratégie de regroupement spéciale pour extraire les harmoniques exponentiellement modulées. Ils proposent une fonction de similarité qui regroupe deux composantes successives $(j, j+1)$ si$$
\frac{1}{2}\max_{0\le k \le L/2}\left(\Pi_{U_j}(k) + \Pi_{U_{j+1}}(k)\right) \ge \rho_0
$$ pour $\rho_0\in \left[0, 1\right]$. Cette métrique rend compte du fait que les composantes successives partage un même pique dans leur périodigramme.


% paragraph  (end)
\paragraph{Abalov method (GG2)}
\label{par:abalov}
\cite{abalov_14_auto}
On utilise
% paragraph abalov (end)
\paragraph{Alvarez}
\label{par:alva}
\cite{alvarez_2013_auto}
% paragraph alva (end)
\paragraph{Notre méthode}
\label{par:tomtom}


        
% paragraph tomtom (end)

% subsection sim (end)

\section{Méthode d'évaluation}
\label{sec:eval}

\subsection{Métrique d'évaluation}
\label{sub:}
Dans \cite{abalov_14_aut}, les auteurs proposent d'utiliser le coéfficient de determination $R^2$ pour évaluter la qualité d'un groupement. Ce coefficient rend compte de l'erreur commise par rapport à la variance de la composante. Pour une serie $f$ de moyenne $\bar f$ et pour un estimateur $\hat f$, le coefficient de determination est calculer suivant$$
R^2(f, \hat f) = 1 - \frac{\|f-\hat f\|^2}{\|f-\bar f\|^2}
$$ La métrique d'évaluation du grouping est alors calculer  en moyennant les valeurs minimales de $R^2$ pour chaque composante $c_i$ du signal d'origine et pour $$
\overline{ R^2} = \sum_{i=1}^n \min_k R^2(c_i, g_k)
$$Cette métrique présente cependant des inconvénients car elle ne rend compte que du rappel de la méthode et non de la précision. En effet, le but de la phase de groupement est d'obtenir des composantes haut niveau. Une des taches essentielles est donc de réussir à filtrer les composantes non utiles. Ainsi, il faut évaluer

% subsection  (end)

\subsection{Séparabilité}
\label{sub:sep}
    L'évaluation de la stratégie de regroupement doit prendre en compte la qualité des composantes obtenue lors de la décomposition du signal. En effet, si les composantes que l'on essaie d'analyser ne sont pas séparables par l'analyse de la matrice de $K$-trajectoire, les stratégies de groupement ne pourront pas améliorer cette séparation.\\
    
    Il est donc important de rappeler ici les propriétés des composantes séparables. Deux signaux $f_1$ et $f_2$ sont dit séparables s'il existe $I_1, I_2 \subset \left \{ 1,\dots, K \right \}$ tel que $I_1\cap I_2 = \emptyset$ et $f_i = \mathcal H(\sum_{j \in I_i} \lambda_j U_j V_j^T)$. Ceci implique notamment l'orthogonalité de toutes les sous séries de longueur $K$ et $L$ de $f_1$ et $f_2$. Différentes illustrations de ces propriétés peuvent être trouvées dans \cite{GNZ_10_SSA}.
% subsection sep (end)


\subsection{Artificial signals}
\label{sub:}

% subsection  (end)
% section eval (end)

\section{Résultats} 

Comparaison avec la transformée en ondelettes empirique (EWT, \cite{gilles_13_EWT}).\\
%Pour le benchmark, je pense:\\
%\begin{itemize}
% \item Générer aléatoirement des signaux avec tendance, partie pseudo harmonique et bruit\\
% $$
% f_t = \sum_i c_i + n_i
% $$$$
% c_0 = trend; \hspace{1cm} c_i = cos(2\pi*w_i*t + u) 
% $$
%\item Faire la décomposition EWT et SSA / comparer les résultats\\
%\item Effectuer les grouping et comparé à la baseline du début\\

%\end{itemize}
%\vspace{1cm}
%Pour les mesures, il faudrait en utiliser deux:\\
%\begin{itemize}
%\item Erreur de reconstruction: $E_r = \sum_t (f_t-\widehat f_t)^2$\\
%où $\widehat f_t = \sum_{i \in I} f^{(i)}$ avec $I = \left \{ i \in [1..K] / \lambda_i > \mu \right \}$\\
%\item Grouping erreur: $E_g= \sum_i \min_j \| c_j - f^{(i)}\|^2$
%\end{itemize}

%Ou alors utiliser des metriques de Separation de source. \cite{vincent_06_BSS}

\bibliographystyle{plain} 
\bibliography{bib_dl}{}
\end{document}
