\documentclass{gretsi}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb}
\usepackage[english,francais]{babel}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
%\usepackage{enumitem}


%Gummi|065|=)
\title{Separation de sources automatique}
\author{Thomas Moreau}
\date{}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother
\def\HH{\mathcal H}

\resumefrancais{Cet article introduit différentes stratégies de groupement automatique pour les composantes issues d'une analyse du spectre singulier (SSA). Cette étape, cruciale à la séparation de composantes haut niveau, permet de séparer les différentes effets présents dans le signal. }
\resumeanglais{This paper introduce automatic grouping strategies for Singular Spectrum Analysis (SSA) components. This step is crucial to get meaningful results}


\newcommand{\R}{\mathbb R}
\newcommand{\serie}[1]{(#1_i)_{1 \le i\le T}}
\newcommand{\val}[3]{(#1_1 #3 \dots #3 #1_#2)}
\newcommand{\set}[1]{\left \{ 1, \dots, #1 \right \}}

\begin{document}

\maketitle


\section{Introduction}

L'analyse du spectre singulier (en anglais SSA : Singular Spectral Analysis) est une technique introduite par Vautard et al \cite{vautard_ghil_89_SSA} permettant d'obtenir de manière simple une décomposition du signal en composantes de tendance, de saisonnalité (harmoniques) et de bruit. Elle est principalement utilisée en finance ou en météorologie où les séries étudiées sont souvent courtes et bruitées. De nombreuses publications ont montré la pertinence de la SSA dans ces contextes, où cette décomposition permet d'apporter de l'information sur les mécanismes produisant ces séries mais aussi de formuler des modèles de prédiction.


Néanmoins, cette technique a été rarement utilisée sur d'autres signaux, principalement à cause de son manque d'automatisation. La pertinence des décompositions obtenues dépend en effet énormément du regroupement des composantes extraites, et résoudre cette tâche de façon manuelle peut s'avérer laborieux. Cet article a pour but de présenter différentes stratégies de regroupement automatique (existantes et originales) ainsi qu'un cadre permettant leur évaluation et leur comparaison.

%Cette technique, basée sur le théorème de Karhunen-Loève, permet de décomposer un signal en une somme de composante haut niveau rattachées à sa tendance et à sa saisonnalité.
%L'analyse de la dynamique sous-jacente d'une série temporelle est très intéressante pour l'étude de phénomènes non stationnaires. .  DEJA DIT NON ?


%\paragraph{Matrices de trajectoires}


\subsection{Rappels sur la SSA}
On considère un signal fini échantillonné $f = \val{f}{T}{,}$ et un entier $K \in \left \{ 1, \dots, T/2 \right \}$. La matrice de K-trajectoire $X^{(K)} \in \R^{K\times L}$ de la série $f$ est défini par:
$$ X^{(K)} = \begin{bmatrix}
	f_1 & f_2 &\dots & f_K\\
	f_2 & f_3 &\dots & f_{K+1}\\
	\dots\\
	f_{L} & f_{L+1} &\dots & f_T
\end{bmatrix}$$ avec $L = T-K$. Cette matrice contient toutes les sous séries de longueur $K$ de $f$.

Le signal peut être reconstruit naturellement à partir de cette matrice du fait de sa structure de Hankel, avec toutes les composantes sur les anti-diagonales égales. Si l'on considère une matrice $M$ n'ayant pas cette structure, le signal ayant la matrice de trajectoire la plus proche de $M$ peut être construit en moyennant le long des anti-diagonales. Cette opération permet de définir un mapping PAS D ANGLICISMES entre l'espace des matrices et celui des signaux. On notera par la suite $\HH: \R^{K\times L} \to \R^T$ l'opération d'hankelisation, qui effectue la moyenne le long des anti-diagonales. Cette opération est linéaire.



Une décomposition en valeur singulière (en anglais SVD pour Singular Value Decomposition) de la matrice de K-trajectoire $X^{(K)}$ de $f$ permet d'extraire les motifs de longueur $K$ les plus intéressants au sens de la variance. En effet, la SVD donne: 
$$X^{(K)} = U \Lambda V^T
$$ avec $U, V$ deux matrices orthogonales et $\Lambda$ une matrice diagonale ayant pour valeurs $\lambda_1\le \dots \lambda_K$. La matrice $U$ peut être vue comme un dictionnaire de motifs de taille $K$ sur lequel est projeté le signal. Ces motifs permettent de capturer au mieux la variance du signal. Les vecteurs singuliers de la SVD maximisent récursivement la covariance de $U_i$ et des sous séries de longueur $K$ de $f$.

%\paragraph{Reconstruction de composantes}


L'étape d'extraction de motif permet d'obtenir une décomposition de la matrice de trajectoire sous la forme d'une somme de matrice de rang 1:$$
X^{(K)} = \sum_{i=1}^K \lambda_i U_iV_i^T 
$$où  $U_i$.Par linéarité de l'opération d'hankelisation, on obtient une décomposition de $f = \sum_{i=1}^K f^{(i)}$ avec $f^{(i)} = \HH(\lambda_iU_iV_i^T)$ la série associée à la matrice de rang 1 $\lambda_iU_iV_i^T$.


\subsection{Regroupement des composantes}

Les oscillations harmoniques ou plus complexes, présentes dans la série initiale sont généralement décomposées sur plusieurs de ces composantes. Le bruit est lui aussi projeté sur plusieurs composantes. Une étape d'identification et de regroupement est alors nécessaire pour produire une décomposition intéréssante. Une sélection des composantes peut être faite en se basant sur les valeurs singulières associées qui rendent compte de l'importance de la composante dans la série. Une étape de regroupement est ensuite nécessaire pour obtenir une décomposition plus simple à analyser.


Un regroupement consiste à fournir un ensemble $(I_k)_{1 \le k \le d}$ tel que $\forall k, j$, $I_k \cap I_j = \emptyset$ and $\displaystyle \cup_{k=1}^d I_k = I$. Les composantes ainsi créées sont alors $f^{(I_k)} = \sum_{i \in I_k} f^{(i)}$. Le but de cette étape est de définir les sous espaces sur lesquels sont projetés les sous séries de longueur $K$ de $f$. Si ces sous espace sont bien choisis, les composantes oscillantes se retrouvent sur une unique composante $f^{(I_k)}$. 



AH NON C PAS JOLI LE PARAGRAPHE SUIVANT, DEVELOPPE ET EXPLIQUE MIEUX AVEC LES MAINS, SANS DONNER LES EQUATIONS MAIS EN DONNANT LE PRINCIPE OU LES HYPOTHESES SOUS JACENTES. ATTENTION AUX CITATIONS, IL NE FAUT PAS DONNER LES NOMS, UNIQUEMENT DECRIRE LE CONTENU
Diverses approches ont été utilisées dans la littérature pour réaliser ce groupement. Si la plupart des travaux Les auteurs de \cite{GNZ_10_SSA} proposent différents indicateurs pour aider au groupement mais ceux-ci requière une sélection manuelle des composantes à grouper.
Les travaux de Alexandrov et Golyandina \cite{alexandrov_05_auto} propose une première approche pour le regroupement automatique, principalement tourner à la récupération d'harmonique pure dans le signal. Dans \cite{alvarez_2013_auto}, Alvarez et al. propose une seconde méthode de regroupement automatique basé sur le clustering des périodigrammes des composantes reconstruites. Cette méthode vise un regroupement plus général des composantes. Dans des travaux récents, Abalov et Gubarev \cite{abalov_14_auto} présentent deux autres stratégies pour obtenir un regroupement automatique. 


% subsection  (end)

\section{Méthodes}
Dans cette section, nous allons présenter plusieurs méthodes BLA BLA BLA.... avec une description unifiée, BLA BLA BLA


\subsection{Formulation générale}
\label{sub:form}

Les stratégies de regroupement peuvent être décrites en trois phases:
\begin{enumerate}
	\item Sélectionner des composantes intéressantes de la SSA
	\item Calculer une matrice d'adjacence entre ces composantes
	\item Regrouper les composantes adjacentes
\end{enumerate}
La première étape est effectée en supprimant les composantes de la SVD ayant une valeur singulière $\lambda_k$ plus faible qu'un certain seuil (que l'on choisira ici adaptatif et de la forme  $\tau_l*\lambda_1$ avec $\tau_l = 0.01$). Ceci permet de filtrer les composantes de bruit en considérant qu'elles ont une part négligeable dans la variance de la série. 

La seconde étape utilise une mesure de similarité entre les composantes afin d'obtenir une matrice d'adjacence. Intuitivement, le choix de cette mesure influence fortement les résultats du regroupement. Il est par exemple parfois pertinent de se  baser sur le contenu fréquentiel des composantes afin d'obtenir un bon regroupement des composantes harmoniques.

Enfin, la troisième étape concerne la stratégie de formation des groupes à partir de la matrice d'adjacence. CA INFLUENCE QUOI PRECISEMENT ?

% subsection form (end)
\subsection{Mesures de similarité}
\label{sub:sim}

\subsubsection{Corrélation}

\paragraph{Corrélation (GG1)}\label{par:GG1}
    Une première mesure de similarité est basée sur la corrélation \cite{abalov_14_auto}. Deux composantes sont regroupées si la corrélation entre elles est plus haute qu'une valeur seuil $\rho_c$. Pour éviter de grouper des composantes d'importance trop éloignée, on ajoute une condition sur le ratio des valeurs singulières associées aux signaux considérés qui doit être proche 1. Cela évite de grouper des phénomènes d'amplitude différentes. On définit la matrice d'adjacence $G = (g_{i, j})_{0 \le i,j\le K}$ par:
    $$
    g_{i, j} = \begin{cases}
	    1 &\text{ si } \displaystyle\frac{\min(\lambda_i, \lambda_j)}{\max(\lambda_i, \lambda_j)} \ge \rho_1 \text{ et } \text{corr}(f^{(i)}, f^{[j)}) \ge \rho_c\\
	    0& \text{ sinon}
    \end{cases}
    $$
% paragraph  (end)

\paragraph{W-Corrélation (GG3)}\label{par:GG3} 
    La w-corrélation pour deux séries $x, y$ de taille $N$ et pour une taille de fenêtre $K$:
    $$
    \text{w-corr}(x, y) = \frac{\langle x|y\rangle_w}{\|x\|_w\|y\|_w}
    $$avec $\langle x|y\rangle_w = \sum_{i=1}^n w_i x_i y_i$, $w_i = \min(i, N-i, K)$ et $\|x\|_w = \sqrt{\langle x|x\rangle_w}$. Deux séries sont séparables par la SSA si leur w-corrélation est nulle \cite{GNZ_10_SSA}. Il est donc possible d'utiliser la valeur de la w-corrélation comme indicateur de similarité entre 2 composantes. On peut alors prendre une fonction de similarité identique à celle de Abalov en remplaçant la corrélation par la w-corrélation. Ceci permet de filtrer les effets de bord qui peuvent apparaître dans la corrélation et ainsi d'obtenir un meilleur regroupement.
% paragraph nn (end)


\subsubsection{Periodigramme}

La fréquence des composantes obtenues peut aussi être utilisé comme un indicateur de similarité des séries. Le périodigrame d'une série $f$ de longueur $N$ est définie comme:$$
\Pi_f(k) = \frac{1}{Z}\left|\sum_{n=1}^N f_n e^{-2i\pi n \frac{k}{N}}\right|^2
$$ où $Z$ est une constante de normalisation telle que le périodigramme est une norme unitaire. Celui-ci peut être utiliser pour choisir un regroupement \cite{GNZ_10_SSA}.

\paragraph{Regroupement harmonique (HG)}\label{par:HG}
    Une stratégie de regroupement efficace pour extraire les harmoniques exponentiellement modulées \cite{alexandrov_05_auto} utilise une fonction de similarité qui regroupe deux composantes successives $(j, j+1)$ si 
    $$
    \frac{1}{2}\max_{0\le k \le L/2}\left(\Pi_{U_j}(k) + \Pi_{U_{j+1}}(k)\right) \ge \rho_0 
    $$pour $\rho_0\in \left[0, 1\right]$. Cette métrique rend compte du fait que les composantes successives partage un même pique dans leur périodigramme.
% paragraph HG (end)


\paragraph{Similarité des piques (GG2)}\label{par:GG2}
    Une autre métrique de similarité basé sur les périodigramme est:
    $$
    g_{i, j} = \begin{cases}
	    1 & \text{ if } \displaystyle\frac{\min(\lambda_i, \lambda_j)}{\max(\lambda_i, \lambda_j)} \ge \rho_1 \text{ et } \text{RF}(f^{(i)}, f^{[j)}) \ge \rho_c\\
	    0 & \text{ sinon}
    \end{cases}
    $$Ici
% paragraph abalov (end)
\paragraph{K-means spéctral (KM)}\label{par:KM}
DAns \cite{alvarez_2013_auto}, Alvarez \& al. proposent d'utiliser une méthode de clustering afin
% paragraph alva (end)


\paragraph{Regroupement harmonique stabilisé (HGS)}\label{par:HGS}


        
% paragraph HGS (end)

\subsection{Stratégie de clustering}
\label{sub:clust}

La stratégie de formation des groupes à partir de la matrice d'adjacence la plus simple est de considérer que toutes les composantes ont un interet uniforme. On peut alors considéré que deux composantes sont dans le même groupe dès lors qu'elles sont adjacentes. L'algorithme \ref{alg:unif} décrite la méthode de formation des groupes dans ce cas la.\\

\begin{algorithm}
\caption{Groupement uniforme}\label{alg:unif}
Given G
\begin{algorithmic}[1]
\For{ i = 1 \dots K}
\State group[i] = i
\EndFor
\For{ i = 1 \dots K}
\For{ j = i+1 \dots K}
\If{$G_{i,j} = 1$} group[j] = group[i]
\EndIf
\EndFor
\EndFor
\State \Return group
\end{algorithmic}
\end{algorithm}

Une autre possibilité est de considéré que les composantes ont une importance différente. L'importance de chaque composante peut être mesurée par la part de variance $\frac{\lambda_i}{\sum_{j=1}^n \lambda_j}$ expliqué par cette composante. On peut donc considéré qu'il est préférable de rajouter une composante $j$ dans un groupe $I_k$ que si $G_{i, j} = 1$ pour $i = \arg\max_{l \in I_k} \lambda_l$. Ici, la composante est ajouté dans un groupe uniquement lorsqu'elle est similaire à la composante la plus importante du groupe.

\begin{algorithm}
\caption{Groupement ordonée}\label{alg:ord}
Given G
\begin{algorithmic}[1]
\State GI $= \left \{  \right \}, I = \set{m}$
\For{$i \in I$}
\State remove i from $I$, $I_i = \left \{ i \right \}$
\For{$j \in I$}
\If{$G_{i,j} = 1$} 
\State remove j from $I$, $I_i = I_i \cup \left \{ j \right \}$
\EndIf
\EndFor
\State GI = GI$ \cup I_i$
\EndFor
\State \Return GI
\end{algorithmic}
\end{algorithm}


% subsection clust (end)

\section{Méthode d'évaluation}
\label{sec:eval}



\subsection{Signaux artificiels}
\label{sub:artsig}
    Les stratégie de grouping testées ont été évaluées sur des signaux artificiels générés aléatoirement. L'objectif du regroupement est de séparere les différentes composantes périodiques de la tendance et du bruit. Les signaux de teste ont été générés pour une fréquence d'échatillonage de $100Hz$ selon le modèle:
    \begin{equation}\label{eq:artsig}
    f(t) = a_0 t^p + \sum_{i=1}^K a_i e^{-\alpha_i t} \sin\left(f_i t + \phi_i\right)
    \end{equation} avec $a_i \sim \mathcal U(0, 1)$, $p \sim \mathcal U(0, 5)$, $\displaystyle \phi_i \sim \mathcal U\left(\frac{-\pi}{2}, \frac{\pi}{2}\right)$, $\alpha_i \sim \mathcal U\left(0, \frac{1}{2}\right)$ et $f_i \sim \mathcal U(0, 50Hz)$ et $\epsilon_t \sim \mathcal N(0, s*std(f))$, $s \sim \mathcal U(3, 30dB)$. Chaque tirage\\
    
     La méthode d'évaluation calcul la moyenne des métriques mentionées ci-dessous pour chaque stratégie de groupement sur 2000 tirages de fonctions selon le modèle (\ref{eq:artsig}).

% subsection  (end)

\subsection{Métrique d'évaluation}
\label{sub:}
Dans \cite{abalov_14_aut}, les auteurs proposent d'utiliser le coéfficient de determination $R^2$ pour évaluter la qualité d'un groupement. Ce coefficient rend compte de l'erreur commise par rapport à la variance de la composante. Pour une serie $f$ de moyenne $\bar f$ et pour un estimateur $\hat f$, le coefficient de determination est calculer suivant$$
R^2(f, \hat f) = 1 - \frac{\|f-\hat f\|^2}{\|f-\bar f\|^2}
$$ La métrique d'évaluation du grouping est alors calculer  en moyennant les valeurs minimales de $R^2$ pour chaque composante $c_i$ du signal d'origine et pour $$
\overline{ R^2} = \sum_{i=1}^n \min_k R^2(c_i, g_k)
$$Cette métrique présente cependant des inconvénients car elle ne rend compte que du rappel de la méthode et non de la précision. En effet, le but de la phase de groupement est d'obtenir des composantes haut niveau. Une des taches essentielles est donc de réussir à filtrer les composantes non utiles \cite{abalov_14_aut}

% subsection  (end)

\subsection{Séparabilité}
\label{sub:sep}
    L'évaluation de la stratégie de regroupement doit prendre en compte la qualité des composantes obtenue lors de la décomposition du signal. En effet, si les composantes que l'on essaie d'analyser ne sont pas séparables par l'analyse de la matrice de $K$-trajectoire, les stratégies de groupement ne pourront pas améliorer cette séparation.\\
    
    Il est donc important de rappeler ici les propriétés des composantes séparables. Deux signaux $f_1$ et $f_2$ sont dit séparables s'il existe $I_1, I_2 \subset \left \{ 1,\dots, K \right \}$ tel que $I_1\cap I_2 = \emptyset$ et $f_i = \mathcal H(\sum_{j \in I_i} \lambda_j U_j V_j^T)$. Ceci implique notamment l'orthogonalité de toutes les sous séries de longueur $K$ et $L$ de $f_1$ et $f_2$. Différentes illustrations de ces propriétés peuvent être trouvées dans \cite{GNZ_10_SSA}.\\
    
   L'évaluation de la qualité du groupement doit donc prendre en compte la qualité des composantes de départ. Pour cela, il est possible de ne considérer que l'amélioration d'un métrique par rapport à cette même métrique calculer pour un grouping trivial tel que $I_k = \left \{ k \right \}$ pour $k \in \set{m}$.
% subsection sep (end)

% section eval (end)

\section{Résultats}

\bibliographystyle{plain} 
\bibliography{bib_dl}{}
\end{document}
